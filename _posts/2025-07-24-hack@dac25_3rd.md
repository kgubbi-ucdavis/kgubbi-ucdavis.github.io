---
title: "AI-Augmented Pre-Silicon Hardware SoC Security Verification: Hack@DAC 2025 Competition Recap"
last_modified_at: 2025-07-22T20:29:27-07:00
categories:
  - blog
tags:
  - Hack@DAC
  - Hardware Security
  - RTL
  - SoC
  - ASIC
  - Generative AI
  - Formal Verification
  - EDA
---

## AI-Augmented Security Verification for RTL: Hack@DAC 2025 Winner

**Date:** July 2025  
**Award:** 3rd Place â€“ Hack@DAC 2025 Hardware Security Challenge  
**Team:** Kevin Immanuel Gubbi (Team Lead), Yuyang Jin, Navid Tarighat, Arvind Sudarshan

### Introduction

As digital systems become more complex, ensuring the security of hardware at the Register Transfer Level (RTL) has become a major challenge. RTL forms the foundation of a chip's functionality, and flaws at this level can introduce subtle vulnerabilities that persist into the final silicon. Unlike software security, where testing and patching are well-established, hardware vulnerabilities can be far more difficult to detect and fix post-fabrication. This makes pre-silicon security verification essential.

However, existing tools often generate hundreds of warnings, many of which are either false positives or of limited relevance to actual security threats. As a result, engineers must manually sift through large volumes of data to determine whether any real threats exist. At Hack@DAC 2025, our team addressed this challenge by developing a semi-automated framework that combines commercial EDA tools with lightweight AI agents to help identify and confirm security vulnerabilities in RTL designs.

### Motivation and Design Strategy

Our goal was not to replace formal verification or security expertise, but to augment the hardware verification process using AI to reduce time and effort. Many commercial verification tools identify issues but do not assess whether those issues are truly exploitable. Our framework builds on existing tools by introducing AI-guided interpretation, classification, and confirmation of security-related bugs. 

We emphasized using AI responsibly and efficiently, focusing on practical augmentation rather than unnecessary compute-intensive operations. The AI models were tightly integrated into the toolchain and guided by structured inputs, enabling consistent and reliable outputs.

### System Components and Workflow

Our pipeline integrated three main components. First, we utilized tools like Synopsys SpyGlass Lint, VC Formal, and TPROP to identify potential security violations. These include uninitialized registers, insecure signal propagation, and control logic vulnerabilities. Tool outputs were parsed and converted into structured data formats suitable for downstream AI processing.

Second, we implemented a signal-level analysis tool that scans Verilog RTL for state-holding elements that may be used without initialization or protection. These issues, often ignored in functional verification, can cause major problems like data leakage or unauthorized state transitions under fault conditions.

The third component is an AI-based testbench generator. Using the vulnerability description and the module interface, the LLM generates simulation testbenches that are targeted at reproducing and confirming suspected vulnerabilities. These testbenches simulate adversarial behavior and check for failure conditions using assertions or trace outputs. The results are stored in a structured format along with CWE mappings and exploitability tags.

By combining formal analysis, static detection, and AI-guided simulation, we created an end-to-end pipeline that can reduce verification time and improve the accuracy of security assessments in RTL.

### Key Results and Contributions

One of our major contributions was the use of LLMs to generate security-focused, simulation-driven testbenches directly from tool outputs. Instead of overwhelming the designer with long reports, our framework filtered relevant warnings and translated them into actionable verification stimuli. This allowed us to confirm whether a warning was a real, user-triggerable vulnerability.

We introduced a lightweight scoring system to quantify confirmed issues using criteria similar to the Common Vulnerability Scoring System (CVSS). This included factors like trigger complexity, control or data impact, and isolation scope. Our system provided a repeatable method to rank vulnerabilities and prioritize fixes.

Throughout the competition, we used this framework to analyze multiple RTL modules and confirmed the presence of critical bugs with real-world security implications. The toolchain accelerated vulnerability triage, reduced false positives, and saved valuable time during testbench development.

### Conclusion

Our Hack@DAC 2025 project demonstrated how generative AI and commercial EDA tools can be effectively combined to enhance RTL-level hardware security verification. By automating the path from detection to confirmation, we addressed one of the biggest gaps in pre-silicon security workflows.

This project reflects a collaborative effort by Kevin Immanuel Gubbi, Yuyang Jin, Navid Tarighat, and Arvind Sudarshan. Together, we showcased how intelligent AI augmentation can streamline hardware verification and help ensure the security of future chip designs. As the industry faces growing demands for trustworthy and secure hardware, tools like ours represent a promising step forward.
